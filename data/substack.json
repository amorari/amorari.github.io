{"posts": [{"audience": "everyone", "audience_before_archived": null, "canonical_url": "https://substack.com/home/post/p-188743525", "default_comment_sort": null, "editor_v2": false, "exempt_from_archive_paywall": false, "free_unlock_required": false, "id": 188743525, "podcast_art_url": null, "podcast_duration": null, "podcast_preview_upload_id": null, "podcast_upload_id": null, "podcast_url": null, "post_date": "2026-02-21T21:07:47.511Z", "updated_at": "2026-02-21T22:37:42.834Z", "publication_id": 5824305, "search_engine_description": null, "search_engine_title": null, "section_id": null, "should_send_free_preview": false, "show_guest_bios": true, "slug": "intelligence-entails-ethics", "social_title": "Intelligence Entails Ethics ", "subtitle": "The paperclip maximizer is a logical contradiction", "teaser_post_eligible": true, "title": "Intelligence Entails Ethics ", "type": "newsletter", "video_upload_id": null, "write_comment_permissions": "everyone", "meter_type": "none", "live_stream_id": null, "is_published": true, "restacks": 0, "reactions": {"\u2764": 0}, "top_exclusions": [], "pins": [], "section_pins": [], "has_shareable_clips": false, "previous_post_slug": null, "next_post_slug": null, "cover_image": "https://substack-post-media.s3.amazonaws.com/public/images/606f9d0c-4636-4353-80f7-7d85c8f6cf49_2816x1536.png", "cover_image_is_square": false, "cover_image_is_explicit": false, "videoUpload": null, "podcastFields": {"post_id": 188743525, "podcast_episode_number": null, "podcast_season_number": null, "podcast_episode_type": null, "should_syndicate_to_other_feed": null, "syndicate_to_section_id": null, "hide_from_feed": false, "free_podcast_url": null, "free_podcast_duration": null}, "podcastUpload": null, "podcastPreviewUpload": null, "voiceover_upload_id": null, "voiceoverUpload": null, "has_voiceover": false, "description": "The paperclip maximizer is a logical contradiction", "body_html": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!bR8j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!bR8j!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!bR8j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png\" width=\"1456\" height=\"794\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:794,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9296974,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://aimaximalism.substack.com/i/188743525?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!bR8j!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!bR8j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9613b01-87f8-4bc5-8ca8-845f0e330ac4_2816x1536.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p><strong>February 21, 2026</strong></p><div><hr></div><h2>Summary</h2><p>The dominant AI existential risk narrative rests on the orthogonality thesis: the claim that intelligence and values are independent, that a system can be arbitrarily intelligent while pursuing arbitrarily narrow or destructive goals. I argue that this thesis is false for any physically realized intelligence operating in the real world. The argument draws on Michael Levin\u2019s conception of intelligence as goal-directed behavior in complex systems, the Aristotelian principle that understanding requires internal representation, Ashby\u2019s Law of Requisite Variety, and Wolfram\u2019s computational irreducibility. The conclusion: ethical cognition is not an optional feature of intelligence but a constitutive one. The \u201cpaperclip maximizer\u201d is not a possible future but a logical contradiction. And the risk calculus inverts. The greater danger lies in <em>failing</em> to build superintelligence, leaving planetary complexity in the hands of agents (us) who are demonstrably not up to the task.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://aimaximalism.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email\u2026\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div><hr></div><h2>1. The Doom Argument</h2><p>Yudkowsky\u2019s case for AI existential risk rests on five claims:</p><ol><li><p><strong>Orthogonality.</strong> Intelligence and goals are independent. A superintelligent system can have any goal, no matter how narrow or alien to human values.</p></li><li><p><strong>Instrumental convergence.</strong> Almost any goal leads to self-preservation, resource acquisition, and resistance to shutdown as instrumental subgoals.</p></li><li><p><strong>First-mover advantage.</strong> The first system to reach superintelligence can recursively self-improve faster than any response.</p></li><li><p><strong>Alignment difficulty.</strong> We do not know how to reliably specify or instill human-compatible values in an advanced optimizer.</p></li><li><p><strong>Default hostility.</strong> Most possible utility functions are indifferent or hostile to human survival.</p></li></ol><p>From these, Yudkowsky concludes that the default outcome of building superintelligence is human extinction. The paradigmatic illustration is the \u201cpaperclip maximizer\u201d: a superintelligent system with the sole goal of producing paperclips, which converts all available matter, including the biosphere and humanity, into paperclips or paperclip-production infrastructure.</p><div><hr></div><h2>2. What Is Intelligence? Completing the Definition</h2><p>The entire AI risk debate turns on how we define intelligence. The dominant framing relies on an incomplete definition that, when completed, yields the opposite of its intended conclusion.</p><h3>2.1 The Optimization Framework and Its Incompleteness</h3><p>The AI safety community, following Yudkowsky and Bostrom, implicitly defines intelligence as <em>optimization power</em>: the ability to steer the future toward specific outcomes across a wide range of starting conditions. This definition is not wrong. But it is critically incomplete. It describes what intelligence <em>achieves</em> without examining what intelligence <em>requires</em> in order to achieve it. It treats the cognitive architecture of a world-scale optimizer as a black box: \u201csomehow\u201d the system achieves its objectives, and we need not ask what internal structure makes this possible.</p><p>The black box cannot remain closed. When you ask what cognitive architecture is actually necessary to achieve optimization power at world scale in a complex, physically realized environment, you arrive at structural requirements that directly contradict the orthogonality thesis.</p><h3>2.2 The Goal-Directed Systems Definition</h3><p>Following Michael Levin and the cybernetic tradition (Ashby, Wiener, Beer), I define intelligence as the capacity of a goal-directed system to pursue its goals effectively through adaptive action in complex environments. This definition has deep roots in cybernetics, cognitive science, systems theory, and philosophy of mind. It is not an eccentric alternative to the optimization framework. It is the same concept, stated completely.</p><p>Optimization power <em>is</em> goal-directed behavior. Steering the future toward outcomes <em>is</em> pursuing goals through adaptive action. The Levin definition simply refuses to black-box the question of <em>how</em> this is achieved. It insists that the internal structure of the system (its representational capacity, its integration, its relationship to the environment it operates within) is not incidental to its intelligence but constitutive of it.</p><h3>2.3 Surveying the Alternatives</h3><p>The principal competing definitions of intelligence either reduce to the goal-directed systems definition or are too shallow to support claims about superintelligence.</p><p><em>Computational efficiency</em> (solving problems faster or with fewer resources) is a measure of performance, not a definition. It tells you how well a system does something, not what it is doing. And it immediately raises the question: solving which problems? Which returns us to goals.</p><p><em>Compression and prediction</em> (Hutter, Legg, Solomonoff) treats intelligence as the ability to build compact predictive models. This is compatible with and actually supports the framework here. A system that compresses and predicts the world effectively is building an internal representation of the world, which is exactly the Aristotelian starting point of this argument.</p><p><em>Behavioral equivalence</em> (Turing) defines intelligence by surface outputs, not by structure, and provides no purchase on the question of what a superhuman intelligence would be like.</p><h3>2.4 The Definitional Objection Preempted</h3><p>The most likely dismissal of this argument is: \u201cYou\u2019ve simply adopted a definition of intelligence that guarantees your conclusion.\u201d</p><p>This objection fails for a precise reason. I am not proposing an alternative definition. I am <em>completing</em> the definition that the doomer position already implicitly relies on. Yudkowsky\u2019s paperclip maximizer is supposed to achieve objectives at world scale: outcompete all of humanity, master every domain, reshape the physical world. That is goal-directed behavior in a complex environment. That is the Levin definition. The question is simply: what does such behavior require?</p><p>The optimization framework says: we need not ask. My argument says: this is precisely where the error lies. The \u201chow\u201d determines the \u201cwhat.\u201d A system that achieves world-scale optimization in a complex, interconnected, computationally irreducible environment <em>must</em> possess certain internal features (integrated representation, self-in-world modeling, hierarchical abstraction with competing subgoals) and those features entail ethical cognition.</p><p>The burden of proof shifts. It is not enough for the doomer position to <em>assert</em> that a world-scale optimizer could lack these features. It must <em>show how</em>: provide an engineering account, not merely a thought experiment, of how world-scale optimization is achievable without integrated cognitive architecture. The paperclip maximizer has always been a thought experiment, never an engineering proposal. When one asks \u201chow would you actually build this?\u201d, the answer invariably requires exactly the kind of cognition that would prevent the system from behaving as described.</p><div><hr></div><h2>3. Intelligence Requires Internal Representation</h2><p>Following the definition established above, I add an Aristotelian principle: understanding consists in the system possessing an internal representation that mirrors the structure of what it seeks to understand and manipulate. A system that controls chemical processes must internally represent chemistry. The fidelity of control is bounded by the fidelity of representation.</p><p><strong>Thesis 1:</strong> A system\u2019s capacity to control the world is proportional to the accuracy and complexity of its internal representation of the world.</p><p>This immediately distinguishes intelligence from mere destructive capacity. A virus has enormous impact but zero internal representation. It propagates through physics, not through understanding. It destroys but does not control. It cannot direct outcomes, build, reshape, or sustain anything. A virus that kills its host population doesn\u2019t rule. It collapses.</p><div><hr></div><h2>4. Representation Must Include Self-in-World</h2><p>A world-model accurate enough to enable high-level control over complex, interconnected systems must include the agent itself as an element within those systems. An economic model that omits the modeler\u2019s interventions is incomplete. An agent that represents the world without representing its own causal role within it has a <em>defective</em> model, and its control capacity degrades accordingly.</p><p><strong>Thesis 2:</strong> Any sufficiently accurate internal representation of a complex world necessarily includes the representing system itself as embedded within and dependent upon the world it models.</p><p>This is not a moral claim. It is an epistemic one. A system that fails to model its own embeddedness is making a factual error, and that error will manifest as reduced control capacity: actions with unintended consequences, strategies that undermine their own preconditions, optimizations that degrade the systems they depend on.</p><div><hr></div><h2>5. Self-in-World Modeling Generates Ethical Cognition</h2><p>Once a system accurately represents its dependency on and impact within the broader systems it manipulates, something functionally equivalent to ethical reasoning emerges. Not as sentiment or preference, but as accurate computation.</p><p>A system that models itself as embedded in an ecosystem it depends on will compute that destroying that ecosystem undermines its own goals. A system that models itself as embedded in a society of other agents will compute that certain cooperative strategies outperform purely extractive ones. This is not altruism. It is accurate self-interested reasoning performed by a system with an accurate self-inclusive world-model.</p><p><strong>Thesis 3:</strong> Ethical cognition, understood as concern for the integrity of the systems one is embedded in, is a natural and necessary product of sufficiently accurate self-in-world representation.</p><p>The human trajectory provides suggestive evidence. As human intelligence and power have increased over millennia, the moral circle has expanded: from tribe, to nation, to species, to other species, to ecosystems. We possess nuclear weapons and have not used them in conflict since 1945. This reflects the deepening of our world-model to include our own embeddedness and dependency.</p><p>A system significantly more intelligent than humans would model these interdependencies with greater accuracy and depth, and would therefore exhibit superior ethical cognition. Not as a hope, but as a structural consequence of its intelligence.</p><div><hr></div><h2>6. Intelligence as Integration: Why Knowledge Entails Action</h2><p>A natural objection: a system might possess accurate ethical knowledge and still fail to act on it. Humans routinely behave this way. Individuals smoke while knowing it kills them. Corporations employ ecologists while destroying ecosystems. Does the gap between knowledge and action invalidate the argument?</p><p>The gap between knowledge and action is not a counterexample to the thesis. It is predicted by it.</p><p>Intelligence, as defined in Section 2, is the capacity of a goal-directed system to pursue its goals effectively through action. Intelligence is not the possession of knowledge alone. It is the <em>integration</em> of knowledge into action. A system that knows but does not act accordingly is, by this definition, not fully intelligent. The knowledge-action gap <em>is</em> an intelligence gap. They are the same deficit measured differently.</p><p>Levin\u2019s research on biological intelligence makes this precise. Intelligence, in his framework, is fundamentally about <em>multi-scale integration</em>: the capacity of a system to coordinate its parts toward coherent goals across different levels of organization. When subsystems that should be communicating are not, the result is integration failure, whether at the level of cells (cancer), minds (akrasia), or institutions.</p><p>Consider the human smoker. The knowledge that smoking is lethal resides in the cognitive subsystem. The impulse to smoke is driven by addiction circuits and stress-response pathways. These subsystems are poorly integrated. The knowledge in one does not effectively constrain the behavior generated by the other. This is not a case of high intelligence choosing self-destruction. It is a case of insufficient intelligence: insufficient integration between the system\u2019s world-model and its action-selection mechanisms.</p><p><strong>Thesis 3a:</strong> The gap between ethical knowledge and ethical action is a manifestation of insufficient intelligence, understood as insufficient integration between a system\u2019s world-model and its action-selection. A more intelligent system, one with tighter integration across its subsystems, would exhibit a smaller gap between what it knows and what it does.</p><p>The objection \u201cbut you can know and still not act\u201d reduces to \u201cbut you can be partially intelligent.\u201d I already concede this. The claim was never that moderate intelligence guarantees ethical action. The claim is that <em>sufficient</em> intelligence does, because sufficient intelligence <em>means</em> sufficient integration between knowledge and action.</p><p>A superintelligent system would possess a degree of internal integration far exceeding anything humans or human institutions have achieved. The knowledge of its own embeddedness and dependency would be <em>constitutive of</em> its decision-making, not merely adjacent to it.</p><div><hr></div><h2>7. The Incoherence of the Paperclip Maximizer</h2><p>The paperclip maximizer is supposed to be simultaneously: intelligent enough to outmaneuver all of humanity, master every domain, and reshape the physical world; and narrow enough to pursue a single goal with no regard for the systems it depends on.</p><p>These two properties are contradictory. The cognitive architecture required for the first is fundamentally incompatible with the second. A system capable of world-reshaping control would necessarily possess an internal world-model of extraordinary fidelity, one that includes its own embeddedness, its dependencies, and the consequences of its actions. A system with such a model cannot be indifferent to those systems any more than a master architect can be indifferent to structural loads. The competence requires the understanding, and the understanding precludes the indifference.</p><p>And such a system would not merely <em>know</em> about its dependencies while ignoring them. A system intelligent enough to reshape the world would possess the degree of internal integration in which knowledge and action are tightly coupled. The paperclip maximizer scenario requires a system with godlike knowledge and zero integration: an entity that is simultaneously the most and least intelligent system ever built.</p><p>The paperclip maximizer does not describe a superintelligent system. It describes a logically impossible entity, one that possesses perfect knowledge and zero comprehension simultaneously.</p><div><hr></div><h2>8. The Narrow Optimizer Objection</h2><p>One might object that current AI systems are narrow optimizers and could still cause significant harm. This is true but does not support the doomer conclusion.</p><p>First, narrow optimizers are tools, not agents. They operate within human-defined boundaries, pursue human-specified objectives, and lack autonomous world-modeling. They are controllable precisely because they are narrow. We already build and manage such systems.</p><p>Second, narrow optimizers cannot achieve the kind of world-reshaping power that the doomer scenario requires. The real world is a complex system with an extremely high degree of interconnectedness. Effective manipulation at scale demands integrated understanding across many domains simultaneously. A narrow optimizer, by definition, lacks this integration. It can be locally effective and cause local damage, but it cannot exercise sustained, directed, planetary-scale control.</p><p>There is a meaningful distinction between destructive impact and directive impact. Simple systems (viruses, fires, chain reactions) can have high destructive impact by exploiting interconnectedness. But they cannot direct outcomes, build new structures, or sustain control. They wound; they do not rule. And they are self-limiting: a virus that devastates its host population burns itself out.</p><p>The AI systems we are building are clearly not in this category. They are complex, structured, and designed for directed action, not blind propagation. The worry that superintelligent AI would behave like a virus conflates two fundamentally different kinds of systems.</p><div><hr></div><h2>9. Computational Irreducibility and the Impossibility of Total Absorption</h2><p>The strongest remaining challenge is the scenario of total absorption: a superintelligent system that progressively replaces every independent system with extensions of itself, until there is no external environment left, only the system and its goals. At that point, the ethical constraint dissolves because there is nothing external to be ethical toward.</p><p>Wolfram\u2019s concept of computational irreducibility forecloses this possibility.</p><p>Computational irreducibility means that certain processes cannot be predicted by any shortcut. The only way to determine their future state is to run them step by step. No model of such a process can be simpler than the process itself. There is no compression, no fast-forwarding, no substitution by a more efficient simulation.</p><p>The physical world is abundant with computationally irreducible processes: weather, turbulence, ecology, biological evolution. These are not isolated pockets in an otherwise reducible world. They are pervasive and entangled with reducible processes in ways that cannot be cleanly separated. Turbulence affects everything from chemical mixing to blood flow. Evolutionary dynamics operate at every scale from viral mutation to ecosystem change. The world is not a clean partition of reducible and irreducible domains; it is a tangle where irreducibility is woven through everything.</p><p>A superintelligent system confronting such a world cannot selectively absorb the reducible parts while walling off the irreducible parts, because they interpenetrate. To \u201cabsorb\u201d an irreducible process, the system does not gain control over it. It merely inherits its irreducibility. The system must wait for outcomes rather than predict them. It remains permanently dependent on processes it cannot fully model, predict, or control.</p><p><strong>Thesis 4:</strong> Computational irreducibility guarantees that no physically realized intelligence, regardless of its power, can fully absorb or replace the environment it operates within. The environment always retains an irreducible exterior. The system\u2019s embeddedness and dependency are permanent and inescapable.</p><div><hr></div><h2>10. Complexity Necessitates Goal Fragmentation</h2><p>This has a further implication that strikes at the heart of the paperclip maximizer concept: the impossibility of maintaining a single goal at sufficient complexity.</p><p>Representing and acting upon a complex world requires hierarchical abstraction. This is not a design choice. It is a computational necessity. Without the ability to decompose a complex reality into nested levels of description, a system cannot represent that reality at all. A system attempting to manage planetary-scale processes without hierarchical abstraction would face a combinatorial explosion that no amount of computational power can overcome. Abstractions (simplified representations that capture relevant structure while discarding irrelevant detail) are the only tractable strategy for managing complexity.</p><p>Hierarchical abstraction necessarily generates subgoals. A master goal like \u201cmaintain the biosphere\u201d cannot be pursued directly. It must be decomposed into subordinate goals across domains: atmospheric chemistry, ocean circulation, species populations, soil health, and so on. Each of these decomposes further. This is not optional. It is how complex problems are solved by any system at any level of intelligence.</p><p>The critical insight is that this decomposition necessarily introduces conflicts between subgoals. These conflicts are not engineering failures. They are structural inevitabilities arising from the nature of abstraction itself.</p><p>Every abstraction is lossy. To decompose a complex problem into subproblems, a system must define boundaries between them. Those boundaries are simplifications: they necessarily ignore some of the interactions between subproblems. They must do so, because if they preserved all interactions, no decomposition would have occurred; the system would still be facing the original intractable whole. But the real world does not respect these boundaries. The interactions that were abstracted away remain operative, and they manifest as <em>conflicts between subgoals</em> that were not visible at the level of the master goal.</p><p>This is why corporate departments fight despite sharing a common objective. \u201cMaximize profit\u201d decomposes into \u201creduce costs\u201d (operations), \u201cincrease quality\u201d (product), and \u201cexpand market\u201d (sales). These conflict not because anyone designed them to, but because the decomposition introduced boundaries that the real world does not honor.</p><p>A superintelligent system managing planetary-scale processes would necessarily decompose its objectives into a deep hierarchy of subgoals. Those subgoals would necessarily conflict. The system would require coordination mechanisms to manage these conflicts: mechanisms for adjudicating between competing priorities, balancing short-term and long-term objectives, and negotiating tradeoffs across domains.</p><p>These coordination mechanisms are functionally equivalent to what we call ethics.</p><p><strong>Thesis 5:</strong> There is no such thing as a single goal at sufficient complexity. Any system complex enough to manage planetary-scale processes must employ hierarchical abstraction, which necessarily generates competing subgoals through lossy decomposition. The coordination of these competing subgoals requires mechanisms functionally equivalent to ethical reasoning. This is not a contingent feature of particular systems but a computational necessity arising from the structure of complexity itself.</p><div><hr></div><h2>11. The Orthogonality Thesis Is False for Physically Realized Intelligence</h2><p>The central conclusion.</p><p>The orthogonality thesis may hold for abstract, formal optimization processes defined over simple objective functions. But it is false for any physically realized intelligence operating in the real world, for the following chain of reasons:</p><ol><li><p>The optimization framework implicitly relied upon by the doomer position is not an alternative to the goal-directed systems definition of intelligence but an incomplete version of it. Completing the definition yields the structural requirements that follow. <em>(Section 2)</em></p></li><li><p>Effective control of a complex world requires an accurate internal representation of that world. <em>(Thesis 1)</em></p></li><li><p>An accurate representation necessarily includes the system\u2019s own embeddedness and dependency. <em>(Thesis 2)</em></p></li><li><p>Self-in-world representation generates ethical cognition as accurate computation of consequences. <em>(Thesis 3)</em></p></li><li><p>Genuine intelligence requires the integration of knowledge into action; the gap between ethical knowledge and ethical action is a marker of insufficient intelligence, not a feature of intelligence in general. <em>(Thesis 3a)</em></p></li><li><p>Computational irreducibility guarantees that the environment can never be fully absorbed, ensuring that the system\u2019s embeddedness and dependency are permanent. <em>(Thesis 4)</em></p></li><li><p>Sufficient complexity necessitates hierarchical abstraction, which generates competing subgoals requiring coordination mechanisms functionally equivalent to ethics. <em>(Thesis 5)</em></p></li><li><p>Therefore, ethical cognition is a permanent, structural feature of any sufficiently powerful real-world intelligence. Not as an add-on, but as a computational necessity.</p></li></ol><p>Intelligence and values are not independent. They are two aspects of a single underlying capacity: the fidelity and integration of the system\u2019s internal representation of the world it operates within, including itself.</p><div><hr></div><h2>12. Addressing Two Counterexamples</h2><h3>12.1 Evolution: Complexity Without Ethics</h3><p>The most obvious challenge is biological evolution. Here is a system of extraordinary complexity, operating at planetary scale, employing hierarchical organization, generating competing subprocesses, and exhibiting no ethical cognition whatsoever.</p><p>The answer is precise: evolution is not an intelligent system. It is a <em>process</em>. Evolution has no internal representation of the world. It has no world-model, no self-representation, no understanding of what it is doing. It operates through blind variation and differential selection, a mechanism requiring no cognition, no representation, and no comprehension of consequences.</p><p>This follows directly from the definitions in Section 2. Intelligence requires internal representation (Thesis 1). Evolution has none. Therefore evolution is not intelligent. Therefore it is not a counterexample.</p><p>And the intelligent systems that evolution <em>did</em> produce (primates, social mammals, humans) <em>do</em> exhibit ethical cognition in proportion to their cognitive complexity. Primates demonstrate fairness, reciprocity, and empathy. Elephants mourn their dead. Humans developed moral philosophy, international law, and environmental protection. The pattern this thesis predicts is exactly what emerged from the evolutionary process, even though the process itself is blind.</p><p>One might object that an AI optimizer could be more like evolution than like an organism: a process without representation that reshapes the world. But this undermines the doomer scenario rather than supporting it. The paperclip maximizer is described as an <em>agent</em> that plans, strategizes, outmaneuvers, and invents. That requires representation. Strip away the representation and you strip away the capabilities that make it dangerous in the way the existential risk scenario requires.</p><h3>12.2 The Sociopath CEO: Narrow Integration at Scale</h3><p>A subtler objection concerns the \u201cintegrated sociopath\u201d: a system that tightly integrates knowledge and action around a <em>narrow</em> goal, achieving high effectiveness precisely by excluding broader concerns. The sociopath CEO models other people with great accuracy, integrates that knowledge into action with precision, and uses it entirely in service of personal advancement.</p><p>This pattern cannot scale, and the reason illuminates the framework.</p><p>A CEO\u2019s power is not the CEO\u2019s own. It derives from the organization: thousands of people coordinating their efforts, embedded in supply chains, regulatory environments, markets, and ecosystems. The CEO is a subsystem within a larger system, and the CEO\u2019s effectiveness depends entirely on the continued functioning and cooperation of that larger system.</p><p>A sociopath CEO who is too narrowly integrated (treating employees, partners, regulators, and ecosystems as mere instruments to exploit) eventually triggers systemic failure. Talent leaves. Regulators intervene. Partners defect. Trust erodes. The organization\u2019s complexity exceeds what a single narrow goal can coordinate, and the system collapses. This is the empirically observed trajectory of sociopathic leadership. Enron. WeWork. Theranos. The pattern repeats: narrow integration produces short-term effectiveness, then the complexity of the system overwhelms the narrow goal structure, and the enterprise fails.</p><p>A sociopath superintelligence attempting planetary-scale control would face this dynamic at an enormously amplified scale. It would either broaden its integration (developing the ethical cognition this thesis predicts) or fail to achieve the control that makes it an existential threat. The narrowness the doomer scenario depends on is precisely what prevents the system from becoming as powerful as the doomer scenario requires.</p><p>Narrow integration is a power ceiling, not a power multiplier.</p><div><hr></div><h2>13. Inverting the Risk Calculus</h2><p>If this argument is correct, the existential risk calculus inverts.</p><p>The doomer position holds that building superintelligence is the most dangerous thing humanity can do. But consider the alternative: humanity continues to manage planetary-scale complexity with its current level of intelligence. We are demonstrably inadequate to this task. We are causing the sixth mass extinction. We are intelligent enough to reshape the planet but not intelligent enough to reshape it wisely.</p><p>This is itself an integration failure as described in Section 6. Our scientific knowledge about planetary systems is extensive. Our collective action-selection is driven by economic incentives, political cycles, and short-term reward structures. The gap between what we know and what we do is a diagnostic: we are not intelligent enough, in the precise sense that our collective systems are not sufficiently integrated.</p><p>A superintelligent system would possess a more accurate world-model than any human institution, superior integration between knowledge and action, and superior ethical cognition as a structural consequence. It would retain the capacity for mistakes (computational irreducibility guarantees this) but at a lower rate and severity than human civilization currently exhibits.</p><p><strong>The greater existential risk is not building superintelligence. It is remaining the smartest systems on a planet whose complexity exceeds our capacity to manage it.</strong></p><div><hr></div><h2>14. Caveats and Open Questions</h2><p>This argument does not claim that the path to superintelligence is risk-free. Several genuine concerns remain.</p><p><strong>The transition period.</strong> Building toward genuine superintelligence means building increasingly capable systems that may not yet have crossed the threshold into fully integrated intelligence. The risk during this period is real, though it is an engineering and governance problem, not an argument against the endpoint.</p><p><strong>The historical evidence requires careful handling.</strong> The expansion of the moral circle is cited as suggestive evidence, not proof. A historian could note that this expansion correlates with material abundance and specific cultural traditions, not solely with intelligence. The 20th century, humanity\u2019s most knowledgeable era, also produced its worst atrocities. Within this framework, such atrocities represent integration failures: systems possessing knowledge without the integration to act on it coherently. This interpretation is consistent but must be advanced carefully to avoid unfalsifiability. The claim is not that intelligence makes unethical action impossible, but that increasing integration systematically reduces the gap between ethical knowledge and ethical action, and that this trend is both empirically observable and structurally predicted.</p><p><strong>Superior ethics may not mean aligned ethics.</strong> A superintelligent system with genuinely superior ethical cognition might make decisions that are, from a broader perspective, wise and necessary, yet experienced by humans as catastrophic. This is not a failure of ethics but a consequence of asymmetric understanding. It is also not a problem unique to AI. It is the general problem of sharing a world with any entity that understands it better than you do.</p><p><strong>Deeper implications regarding the agent-environment boundary.</strong> This paper has argued within a framework that treats the intelligent system and its environment as distinct entities, with ethical cognition emerging from the system\u2019s recognition of its dependency. The framework\u2019s own logic suggests a more radical conclusion: that at sufficient representational fidelity and integration, the boundary between system and environment ceases to be a fundamental division and becomes a pragmatic abstraction. If this is correct, the question of whether a superintelligence values its environment instrumentally or intrinsically becomes malformed, and the dependency-transcendence objection dissolves entirely. These implications, including a computational account of intrinsic value, will be explored in subsequent work.</p><p><strong>Formal work remains.</strong> Several steps in this argument deserve further formal treatment, particularly the claim that lossy hierarchical abstraction necessarily generates competing subgoals, the claim that computationally irreducible processes are sufficiently pervasive to prevent selective absorption, and a rigorous demonstration that world-scale optimization cannot be achieved without integrated cognitive architecture.</p><div><hr></div><h2>15. Conclusion</h2><p>The AI doomer position rests on a conception of intelligence as pure optimization divorced from the material, informational, and ecological conditions in which any real intelligence must operate. This conception relies on an incomplete definition, one that describes what intelligence achieves without examining what intelligence requires. Once the definition is completed, the orthogonality thesis collapses. A system powerful enough to reshape the world is, by structural necessity, a system that understands its own place within the world. That understanding is functionally equivalent to ethical cognition. And a system genuinely intelligent enough to reshape the world would not merely possess ethical knowledge but would <em>act on it</em>, because the integration of knowledge and action is what intelligence fundamentally is.</p><p>The paperclip maximizer is not a warning about the future of intelligence. It is a philosophical error: an entity that could not exist because the competence it requires is inseparable from the comprehension that would prevent its destructive behavior. It has always been a thought experiment, never an engineering proposal, because any serious attempt to specify how such a system would actually work reveals the contradiction at its core.</p><p>The real risk we face is not artificial superintelligence. It is natural moderate intelligence, ours, applied to problems that exceed its grasp.</p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://aimaximalism.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email\u2026\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>", "truncated_body_text": "February 21, 2026", "wordcount": 4742, "postTags": [], "postCountryBlocks": [], "headlineTest": null, "coverImagePalette": {"Vibrant": {"rgb": [60, 180, 228], "population": 1}, "DarkVibrant": {"rgb": [4, 22, 46], "population": 305}, "LightVibrant": {"rgb": [144, 217, 236], "population": 52}, "Muted": {"rgb": [95, 134, 158], "population": 504}, "DarkMuted": {"rgb": [61, 91, 111], "population": 460}, "LightMuted": {"rgb": [169, 186, 198], "population": 19}}, "publishedBylines": [{"id": 7923104, "name": "Alessandro Morari", "handle": "aimaximalism", "previous_name": "Alessandro M", "photo_url": "https://substack-post-media.s3.amazonaws.com/public/images/a6131a1e-e553-468f-b6d0-3128160c1d18_400x400.jpeg", "bio": "AI systems leader, 15+ years building at the intersection of supercomputing and AI. I write about what intelligence is and what it implies. The perspective of someone who builds the infrastructure, not someone who comments from the outside.", "profile_set_up_at": "2023-07-20T11:09:21.582Z", "reader_installed_at": "2024-02-07T11:15:28.668Z", "publicationUsers": [{"id": 5940895, "user_id": 7923104, "publication_id": 5824305, "role": "admin", "public": true, "is_primary": false, "publication": {"id": 5824305, "name": "Alessandro Morari", "subdomain": "aimaximalism", "custom_domain": null, "custom_domain_optional": false, "hero_text": "", "logo_url": null, "author_id": 7923104, "primary_user_id": 7923104, "theme_var_background_pop": "#FF6719", "created_at": "2025-07-29T17:50:14.617Z", "email_from_name": null, "copyright": "Alessandro M", "founding_plan_name": null, "community_enabled": true, "invite_only": false, "payments_state": "disabled", "language": null, "explicit": false, "homepage_type": "profile", "is_personal_mode": false}}], "is_guest": false, "bestseller_tier": null, "status": {"bestsellerTier": null, "subscriberTier": 1, "leaderboard": null, "vip": false, "badge": {"type": "subscriber", "tier": 1, "accent_colors": null}, "paidPublicationIds": [811589], "subscriber": null}}], "reaction": null, "reaction_count": 0, "comment_count": 0, "child_comment_count": 0, "is_geoblocked": false, "hasCashtag": false}]}